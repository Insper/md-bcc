{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Databases\n",
    "\n",
    "Em **NLP** (*Natural Language Processing*), frases e documentos são frequentemente representados como **vetores numéricos**. Isto ocorre, por exemplo, para prover integração com *Machine Learning* (modelos de **NLP** frequentemente produzem ou utilizam vetores em suas operações) e pesquisa eficiente por similaridade (ao buscar documentos ou frases semelhantes).\n",
    "\n",
    "Um banco de dados vetorial (*Vector Database*) ou mecanismo de busca vetorial é um **banco de dados** que pode armazenar tais **vetores** (listas de números de comprimento fixo) juntamente com outros itens de dados, enquanto permite recuperar informação de forma **eficiente** e **escalável**.\n",
    "\n",
    "Nesta aula, exploraremos desde a representação vetorial de textos até uma aplicação de **RAG** (*Retrieval-Augmented Generation*) com **LLM** (*Large Language Model*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalação das Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chave OpenAI\n",
    "\n",
    "Nesta aula, iremos utilizar a API da OpenAI. Utilize a chave fornecida pelo professor.\n",
    "\n",
    "**ATENÇÃO**: Utilize esta chave com cuidado. Não desperdice recursos, não execute células múltiplas vezes de forma desnecessária e, mais importante, **NÃO VAZE A CHAVE** publicamente nem compartilhe com terceiros!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Digite a chave da OpenAI:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textos: Representação Vetorial\n",
    "\n",
    "Vetores são representações matemáticas de dados em um espaço de alta dimensão. Nesse espaço, cada dimensão corresponde a uma característica dos dados, com o número de dimensões variando de algumas centenas a dezenas de milhares, dependendo da complexidade dos dados representados.\n",
    "\n",
    "A maneira mais simples de vetorizar textos é utilizando o método **Bag of Words** (**BoW**). Nele, cada texto é representado pela ocorrência (ou frequência) de suas palavras. Ele ignora a ordem das palavras e considera apenas a presença ou ausência delas no documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por exemplo, vamos supor um dicionário de uma língua com apenas três palavras:\n",
    "\n",
    "```python\n",
    "dic = [\"oi\", \"bom\", \"horrível\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, considere as seguintes frases:\n",
    "\n",
    "```python\n",
    "frases = [\n",
    "    \"oi tudo bom?\",\n",
    "    \"bom dia\",\n",
    "    \"foi horrível\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos fazer uma representação vetorial das frases pelos seguintes vetores:\n",
    "\n",
    "```python\n",
    "vetores = [\n",
    "    [1, 1, 0], # Tem palavra \"oi\", tem \"bom\", não tem \"horrivel\"\n",
    "    [0, 1, 0], # Não tem \"oi\", tem \"bom\", não tem \"horrível\"\n",
    "    [0, 0, 1], # Não tem \"oi\", não tem \"bom\", tem \"horrível\"\n",
    "]\n",
    "```\n",
    "\n",
    "Perceba que ignoramos as palavras não pertencentes ao dicionário.\n",
    "\n",
    "Vamos representar isto em código-fonte!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Frases personalizadas\n",
    "frases = [\n",
    "    \"oi tudo bom?\",\n",
    "    \"bom dia\",\n",
    "    \"foi horrível\",\n",
    "    \"oi, tive um dia bom e horrível ao mesmo tempo\"\n",
    "]\n",
    "\n",
    "# Vetores das frases, considerando o dicionário [\"oi\", \"bom\", \"horrível\"]\n",
    "vetores = np.array([\n",
    "    [1, 1, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [1, 1, 1],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E visualizar um gráfico dos vetores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cores diferentes para cada ponto\n",
    "cores = [\"#636EFA\", \"#EF553B\", \"#00CC96\", \"#666666\"]\n",
    "\n",
    "# Criar uma figura do Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adicionar pontos ao gráfico\n",
    "for vector, color, frase in zip(vetores, cores, frases):\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[vector[0]],\n",
    "        y=[vector[1]],\n",
    "        z=[vector[2]],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=5, color=color),\n",
    "        text=f\"{frase} {vector}\",\n",
    "        hoverinfo=\"text\"\n",
    "    ))\n",
    "\n",
    "# Definir título e rótulos dos eixos com limites\n",
    "fig.update_layout(\n",
    "    title=\"Vetores de Frases - Gráfico de Pontos 3D\",\n",
    "    scene=dict(\n",
    "        xaxis=dict(title=\"Eixo oi\", range=[0, 2]),\n",
    "        yaxis=dict(title=\"Eixo bom\", range=[0, 2]),\n",
    "        zaxis=dict(title=\"Eixo horrível\", range=[0, 2])\n",
    "    ),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Mostrar o gráfico\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similaridade entre vetores\n",
    "\n",
    "A similaridade do cosseno mede o cosseno do ângulo entre dois vetores. É uma medida popular em processamento de linguagem natural e recuperação de informações e pode ser calculada como:\n",
    "\n",
    "\n",
    "$$ s(A,B)=\\frac{A⋅B}{∥A∥∥B∥}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja uma definição em python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    cosine_sim = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dica**: Também é possível utilizar a função da biblioteca `scipy`:\n",
    "\n",
    "```python\n",
    "from scipy.spatial.distance import cosine\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos calcular a similaridade entre as frases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular a matriz de distância cosseno\n",
    "num_frases = len(frases)\n",
    "matriz_distancia = np.zeros((num_frases, num_frases))\n",
    "\n",
    "for i in range(num_frases):\n",
    "    for j in range(num_frases):\n",
    "        matriz_distancia[i, j] = cosine_similarity(vetores[i], vetores[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E visualizar o mapa de calor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma figura do Plotly\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=matriz_distancia,\n",
    "    x=frases,\n",
    "    y=frases,\n",
    "    colorscale=\"YlOrRd\"\n",
    "))\n",
    "\n",
    "# Definir título e rótulos dos eixos\n",
    "fig.update_layout(\n",
    "    title=\"Matriz de Distância Cosseno\",\n",
    "    xaxis_title=\"Frases\",\n",
    "    yaxis_title=\"Frases\"\n",
    ")\n",
    "\n",
    "# Mostrar o gráfico\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício**: A frase `\"bom dia\"` é mais similar à frase `\"oi tudo bom?\"` ou à frase `\"foi horrível\"`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua resposta aqui!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício**: Qual seria a representação vetorial da frase `\"olá tenham todos um dia horrível\"` considerando o mesmo dicionário do exemplo?\n",
    "\n",
    "<a href=\"#\" title=\"[0, 0, 1] pois apenas a palavra 'horrivel' está no dicionário e no texto!\">Passe o mouse aqui para conferir resposta.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua resposta aqui!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indo além do BoW: Word Embeddings\n",
    "\n",
    "Embora seja simples e eficiente, o **BoW** tem várias limitações. Primeiramente, ele não captura a semântica das palavras. Por exemplo, as palavras `\"cão\"` e `\"cachorro\"` são sinônimos, mas no **BoW**, elas são tratadas como palavras completamente diferentes.\n",
    "\n",
    "Além disso, **BoW** também não considera a **ordem das palavras**, o que pode ser problemático para entender o contexto. Por exemplo, as frases `\"o gato mordeu o cachorro\"` e `\"o cachorro mordeu o gato\"` têm significados muito diferentes, mas no **BoW**, elas podem ser representadas da mesma forma se contiverem as mesmas palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esses problemas levam à necessidade de representações de texto mais avançadas, que capturam a semântica e o contexto das palavras. É aqui que os *embeddings* entram em cena. *Embeddings* são representações vetoriais densas onde **palavras com significados semelhantes têm representações semelhantes**.\n",
    "\n",
    "<img src=\"img/embedding.png\">\n",
    "\n",
    "\n",
    "Diferentemente do **BoW**, que representa cada palavra como um valor único em um vetor esparso, os *embeddings* mapeiam palavras para um espaço **vetorial contínuo** de relativa baixa dimensão. Isso permite que relações semânticas e contextuais sejam preservadas.\n",
    "\n",
    "Por exemplo, em um espaço de *embeddings*, as palavras `\"rei\"` e `\"rainha\"` estarão próximas uma da outra, refletindo sua semelhança semântica, enquanto estarão longe de uma palavra como `\"carro\"`.\n",
    "\n",
    "<style>\n",
    "    .image-container {\n",
    "        background-color: white;\n",
    "        display: inline-block;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"image-container\">\n",
    "    <img src=\"img/emb_vectors.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os modelos utilizados para *embedding* são geralmente treinados em grandes corpora de texto. Esses modelos aprendem as representações vetoriais das palavras com base em seu contexto de uso nas frases, capturando assim **nuances semânticas** e **sintáticas** que **BoW** não consegue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dica**: os modelos modernos de redes neurais em **NLP** processam **tokens** ao invés de palavras. Um token é a menor unidade de texto que pode ser processada pelo modelo. Tokens podem ser palavras, caracteres, sinais de pontuação, símbolos ou partes de palavras.\n",
    "\n",
    "Acesse o link https://platform.openai.com/tokenizer e experimente o tokenizador online da OpenAI. Digite alguns textos, com pontuações, e observe os tokens gerados.\n",
    "\n",
    "<img src=\"img/openai_tokens.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Embeddings\n",
    "\n",
    "Vamos utilizar a API da OpenAI para realizar o embedding de textos.\n",
    "\n",
    "Veja mais em https://platform.openai.com/docs/guides/embeddings\n",
    "\n",
    "Podemos especificar o modelo com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "model = \"text-embedding-ada-002\"\n",
    "embedding_model = OpenAIEmbeddings(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E utilizar para fazer o embedding de uma frase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetor = embedding_model.embed_query(\"meu cahorro é muito fofo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conferindo o resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"O vetor tem {len(vetor)} dimensões\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos definir algumas frases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frases_novas = [\n",
    "    \"meu cachorro é muito fofo\",\n",
    "    \"iphone 15 pro max é muito caro\",\n",
    "    \"quero um pet que é amigo das crianças\",\n",
    "    \"smartphones estão focando em IA\",\n",
    "    \"apple encerra programa de carro autônomo\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E utilizar programação funcional para fazer o embedding de cada frase.\n",
    "\n",
    "**Dica**: poderíamos ter utilizado laço `for`, utilizar programação funcional é só um detalhe aqui! Para cada frase, será mapeada a função `embedding_model.embed_query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vetores_novos = list(map(embedding_model.embed_query, frases_novas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtemos como resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"A matriz possui {len(vetores_novos)} vetores\")\n",
    "print(f\"Cada vetor possui {len(vetores_novos[0])} dimensões\")\n",
    "print(f\"Ou seja, temos uma matriz {len(vetores_novos)}x{len(vetores_novos[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos calcular a similaridade entre as frases pelo cálculo da similaridade cosseno entre os seus vetores:\n",
    "\n",
    "Confira quais frases são mais similares entre si."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "similarity_matrix = cosine_similarity(vetores_novos)\n",
    "\n",
    "fig = px.imshow(similarity_matrix, \n",
    "                x=frases_novas, \n",
    "                y=frases_novas, \n",
    "                color_continuous_scale=\"Viridis\",\n",
    "                labels={\"x\": \"Frases\", \"y\": \"Frases\", \"color\": \"Similaridade\"})\n",
    "fig.update_layout(title=\"Heatmap de Similaridades Cosseno\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busca\n",
    "\n",
    "Vamos supor que possuímos muitos textos representados de forma vetorial. Os textos poderiam ser parágrafos de diversos livros ou notícias. Se o usuário tiver alguma questão e desejar encontrar os vetores mais semelhantes à sua pergunta. Por exemplo:\n",
    "\n",
    "```python\n",
    "\"Quais são os objetivos da área de tecnologia em 2024?\"\n",
    "```\n",
    "\n",
    "Como isto poderia ser feito?\n",
    "\n",
    "**Exercício**: Como você faria para realizar esta busca? Quais seriam os passos?\n",
    "\n",
    "<a href=\"#\" title=\"1) fazer embedding do texto da pergunta 2) calcular a distância entre o vetor do texto da pergunta e todos os vetores armazenados 3) retornar os mais semelhantes.\">Passe o mouse aqui para conferir resposta.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua resposta AQUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício**: Comparar o vetor da pergunta com todos os vetores é eficiente de um ponto de vista computacional?\n",
    "\n",
    "<a href=\"#\" title=\"Não! Pois seria O(kn), onde k é o tamanho do vetor e n é a quantidade de vetores. Apesar de ser linear no número de vetores, seria ineficiente ao considerar um número elevado de vetores.\">Passe o mouse aqui para conferir resposta.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua resposta AQUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício**: Utilizar um SGBD relacional poderia ajudar? Pense em como um SGBD realiza buscas e qual seria o impacto nesta situação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua resposta AQUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChromaDB\n",
    "\n",
    "Da mesma maneira que **SGBDs** como o **MySQL** implementam uma série de recursos para trabalhar com dados segundo o modelo relacional, também temos opções para bancos de dados vetoriais.\n",
    "\n",
    "Nesta aula, iremos utilizar o [**Chroma DB**](https://www.trychroma.com/), mas temos outras opções comumente utilizadas:\n",
    "\n",
    "- Pinecone (cloud)\n",
    "- Milvus\n",
    "- Qdrant\n",
    "- PostgreSQL (um SGBD com módulo pgVector para vector DB)\n",
    "\n",
    "<img src=\"img/chroma.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como exemplo, vamos utilizar o livro da Alice no País das Maravilhas (`data/alice.txt`).\n",
    "\n",
    "Vamos dividir o livro em pedaços ou *chunks*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "chunk_size = 300\n",
    "\n",
    "raw_documents = TextLoader(\"data/alice.txt\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0, separator=\"\\n\")\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja um documento gerado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então, criamos o banco de dados. Observe que iremos aplicar o `embedding_model`, que irá requerer chamadas à API da OpenAI.\n",
    "\n",
    "Ainda, será criada uma pasta `alice_chroma_*`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "db = Chroma.from_documents(\n",
    "    documents, embedding_model, persist_directory=f\"./alice_chroma_{model}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comente a linha acima que cria o `db` para evitar chamadas desnecessárias à API da OpenAI. Vamos passar a abrir direto do arquivo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "db = Chroma(\n",
    "    embedding_function=embedding_model, persist_directory=f\"./alice_chroma_{model}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos utilizar o `db` para recuperar textos cujos vetores sejam mais similares à determinada pergunta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pergunta = \"Qual a cor das rosas da grande roseira?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para isto, será necessário calcular o vetor da pergunta (utilizando o mesmo modelo de *embedding* aplicado aos dados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_pergunta = embedding_model.embed_query(pergunta)\n",
    "\n",
    "print(f\"O vetor da pergunta tem dimensão {len(embedding_pergunta)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então, podemos encontrar os `k` vetores mais similares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_resposta = db.similarity_search_by_vector(embedding_pergunta, k=3)\n",
    "\n",
    "for doc in docs_resposta:\n",
    "    # Exibir até 100 primeiros caracteres do conteúdo\n",
    "    print(doc.page_content[:100])\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercício**: teste com outras perguntas! Confira no arquivo `data/alice.txt` se as respostas fazem sentido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua resposta AQUI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por enquanto apenas retornamos os textos cujos vetores são mais semelhantes à pergunta. O texto é um recorte do texto original e o usuário precisa ler os textos e procurar a resposta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "**RAG** (**Retrieval Augmented Generation**), é uma técnica que combina **recuperação de informações** com um modelo **LLM** de geração de texto para melhorar os resultados. Este método fornece ao modelo informações que podem estar mais atualizadas do que o conjunto de dados usado no treinamento.]\n",
    "\n",
    "Ainda, pode prover informações espefícicas de um contexto, por exemplo, quando queremos respostas envolvendo apenas os dados de um PDF empresarial privado.\n",
    "\n",
    "A estrutura de uma solução RAG envolve três etapas principais:\n",
    "\n",
    "1) Identificação de documentos relevantes que refletem o contexto da pergunta\n",
    "2) Combinação desse contexto com um prompt que contém instruções específicas\n",
    "3) geração do texto utilizando um LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso prompt será construído com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Você é um assistente para tarefas de resposta a perguntas. Use as seguintes partes do contexto recuperado para responder à pergunta. Se você não sabe a resposta, basta dizer que não sabe. Use no máximo três frases e mantenha a resposta concisa.\n",
    "\n",
    "Pergunta: {question} \n",
    "\n",
    "Contexto: {context} \n",
    "\n",
    "Resposta:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos especificar onde os textos de contexto são recuperados. Iremos utilizar o `db` (Chroma DB construído anteriormente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como **LLM**, iremos utilizar o modelo `gpt-3.5-turbo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E criar uma *RAG chain* com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então, podemos fazer nossas perguntas, que serão respondidas utilizando o texto da Alice como referência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"Qual o título do livro? E do terceiro capítulo?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar que de fato o contexto é importante, vamos criar um *retriever* vazio e tentar repetir a pergunta. Perceba que, como não temos os dados do livro da Alice, a API do ChatGPT não irá responder corretamente (faltou contexto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_retriever = Chroma(embedding_function=embedding_model).as_retriever()\n",
    "\n",
    "rag_chain_no_retriever = (\n",
    "    {\"context\": empty_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_no_retriever.invoke(\"Qual o título do livro? E do terceiro capítulo?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dica:** Os prompts podem ser obtidos direto do *hub* langchain:\n",
    "\n",
    "```python\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "```\n",
    "\n",
    "Veja mais em https://smith.langchain.com/hub/rlm/rag-prompt e https://smith.langchain.com/hub/rlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chainlit\n",
    "\n",
    "Vamos criar um *app* para conversar com o livro da Alice utilizando o *chainlit* https://docs.chainlit.io/.\n",
    "\n",
    "Rode a célula abaixo para criar um arquivo `app.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "import chainlit as cl\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "def singleton(cls):\n",
    "    instances = {}\n",
    "\n",
    "    def get_instance(*args, **kwargs):\n",
    "        if cls not in instances:\n",
    "            instances[cls] = cls(*args, **kwargs)\n",
    "        return instances[cls]\n",
    "\n",
    "    return get_instance\n",
    "\n",
    "\n",
    "@singleton\n",
    "class MyRAG:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        def format_docs(docs):\n",
    "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "        embedding_model_name = os.getenv(\"EMBEDDING_MODEL\")\n",
    "        embedding_model = OpenAIEmbeddings(model=embedding_model_name)\n",
    "\n",
    "        num_vectors = int(os.getenv(\"NUM_VECTORS\"))\n",
    "\n",
    "        vector_db_dir = os.getenv(\"VECTOR_DB_DIR\")\n",
    "\n",
    "        retriever = Chroma(\n",
    "            embedding_function=embedding_model,\n",
    "            persist_directory=vector_db_dir,\n",
    "        ).as_retriever(num_vectors=num_vectors)\n",
    "\n",
    "        prompt_template = \"\"\"\n",
    "Você é um assistente para tarefas de resposta a perguntas. Use as seguintes partes do contexto recuperado para responder à pergunta. Se você não sabe a resposta, basta dizer que não sabe. Use no máximo três frases e mantenha a resposta concisa.\n",
    "\n",
    "Pergunta: {question} \n",
    "\n",
    "Contexto: {context} \n",
    "\n",
    "Resposta:\n",
    "\"\"\"\n",
    "\n",
    "        prompt = PromptTemplate(\n",
    "            template=prompt_template, input_variables=[\"question\", \"context\"]\n",
    "        )\n",
    "\n",
    "        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "        self.rag_chain = (\n",
    "            {\n",
    "                \"context\": retriever | format_docs,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def invoke(self, pergunta):\n",
    "        return self.rag_chain.invoke(pergunta)\n",
    "\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def on_chat_start():\n",
    "    await cl.Message(\n",
    "        content=\"Oi!\\nSou a assistente virtual da Alice no País das Maravilhas!\\nMe faça perguntas sobre o livro!\"\n",
    "    ).send()\n",
    "\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message: cl.Message):\n",
    "    resp = MyRAG().invoke(message.content)\n",
    "    await cl.Message(content=resp).send()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preencha o arquivo `.env`a partir do `.env.example` (faça uma cópia).\n",
    "\n",
    "Então, no terminal, inicialize a aplicação chainlit com:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```console\n",
    "chainlit run app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acesse, em seu navegador, a URL fornecida, provavelmente http://localhost:8000\n",
    "\n",
    "Pronto, você criou uma aplicação RAG!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Vector_database\n",
    "- https://platform.openai.com/docs/guides/embeddings\n",
    "- https://platform.openai.com/tokenizer\n",
    "- https://python.langchain.com/v0.1/docs/use_cases/question_answering/sources/\n",
    "- https://semaphoreci.com/blog/word-embeddings\n",
    "- https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/\n",
    "- Imagens:\n",
    "    - https://docs.trychroma.com/img/hrm4.svg\n",
    "    - https://storage.googleapis.com/gweb-cloudblog-publish/images/image4_fUvNRO7.max-800x800.png\n",
    "- Livro: http://www.ebooksbrasil.org/eLibris/alicep.html autorizado para uso didático conforme http://www.ebooksbrasil.org/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
